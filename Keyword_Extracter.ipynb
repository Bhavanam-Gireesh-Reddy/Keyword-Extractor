{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ce65ca-7bf6-4eeb-9be7-a275d2c180f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Import LangChain document loaders\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, PyPDFLoader\n",
    "# UnstructuredWordDocumentLoader is not directly in community.document_loaders, it's typically part of unstructured,\n",
    "# but can be used via UnstructuredFileLoader or directly if specifically installed.\n",
    "# For simplicity, we'll use a more general approach or rely on unstructured's auto-detection.\n",
    "# Or, if you need explicit UnstructuredWordDocumentLoader:\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader # This can handle docx, pdf, txt, etc. based on file extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1f9a04-8e8e-4f29-b444-d00328a9d0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d4030f-aed2-49c6-862e-a96f232c1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM and Prompt Setup ---\n",
    "\n",
    "# 1. Initialize your LLM with Groq\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not found in environment variables. Please set it in your .env file or as an environment variable.\")\n",
    "\n",
    "llm = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\", temperature=0, api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70854714-ba72-4c0e-bea1-cbbfb8eebd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define your prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"From the following text, extract all important keywords and key phrases. \"\n",
    "            \"Focus on domain-specific terminology, named entities, and core concepts. \"\n",
    "            \"Format your response as a comma-separated list of phrases.\"\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2feb30-324c-4af1-b15d-828f65888ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a keyword extraction chain\n",
    "keyword_extraction_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- Input Handling Functions using LangChain Loaders ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c7a540d-54cc-40cf-9f0b-027c3115092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Universal Keyword Extractor!\n",
      "You can provide text directly, a file path (PDF, DOCX, TXT), or a URL.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your text, file path, or URL:  C:\\Users\\hp\\Downloads\\Gireesh Resume.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected PDF file: C:\\Users\\hp\\Downloads\\Gireesh Resume.pdf\n",
      "Split article into 5 chunks.\n",
      "\n",
      "Processing Chunk 1/5\n",
      "Keywords from Chunk 1: Here are the important keywords and key phrases extracted from the text as a comma-separated list:\n",
      "\n",
      "C/C++, Java, Python, SQL, AI, ML, DL, NN, CNN, NLP, LLM, RAG, Git, GitHub, API, PowerBI, transformer-based deepfake detection model, deepfake detection, data engineering, data collection, data processing, Timesformer Model, Hugging-face, roop-unleashed, face-swapping model, Pi-labs.ai, ML & Data Engineering Intern.\n",
      "\n",
      "Let me know if you'd like me to help with anything else! \n",
      "\n",
      "Alternatively, I can also provide the list in a more categorized format:\n",
      "\n",
      "**Programming Languages:** C/C++, Java, Python, SQL\n",
      "**Technologies & Tools:** AI, ML, DL, NN, CNN, Git, GitHub, API, PowerBI, NLP, LLM, RAG\n",
      "**Domain-specific Concepts:** deepfake detection, data engineering, transformer-based deepfake detection model\n",
      "**Models & Frameworks:** Timesformer Model, Hugging-face, roop-unleashed\n",
      "**Organizations:** Pi-labs.ai \n",
      "**Roles:** ML & Data Engineering Intern\n",
      "\n",
      "Processing Chunk 2/5\n",
      "Keywords from Chunk 2: Here are the important keywords and key phrases extracted from the text as a comma-separated list:\n",
      "\n",
      "Hugging Face, Roop-unleashed, Face Swapping Tool, Data Engineering, yt-dlp, ML Internship, Calorie prediction model, Random Forest, Fitbit activity data, Streamlit, Joblib, Python, Pandas, Scikit-learn, Data Visualization, Symbiosis Institute Of Technology, Bachelor of Engineering, Computer Science, AI & ML, Object Oriented Programming, Databases, Discrete Mathematics, Data Structures and Algorithms, Operating Systems, Computer Networks, Machine Learning, Deep Learning, Data Science, Cloud Computing, Deepfake Detection, Data Engineering Techniques.\n",
      "\n",
      "Let me know if you'd like me to help with anything else! \n",
      "\n",
      "Alternatively, I can also group them by category for better readability:\n",
      "\n",
      "**Tools and Technologies:** Hugging Face, Roop-unleashed, Face Swapping Tool, yt-dlp, Streamlit, Joblib, Python, Pandas, Scikit-learn\n",
      "\n",
      "**Concepts and Techniques:** Data Engineering, ML Internship, Calorie prediction model, Random Forest, Data Visualization, Object Oriented Programming, Machine Learning, Deep Learning, Data Science, Cloud Computing\n",
      "\n",
      "**Education and Institutions:** Symbiosis Institute Of Technology, Bachelor of Engineering, Computer Science, AI & ML\n",
      "\n",
      "**Projects and Courses:** Design and Development of Data Engineering Techniques for Deepfake Detection, Relevant Coursework ( Databases, Discrete Mathematics, Data Structures and Algorithms, Operating Systems, Computer Networks, Machine Learning, Deep Learning, Chemistry, Physics) \n",
      "\n",
      "Let me know if you have a preference!\n",
      "\n",
      "Processing Chunk 3/5\n",
      "Keywords from Chunk 3: Here are the extracted keywords and key phrases in a comma-separated list:\n",
      "\n",
      "Deepfake detection, TimeSformer transformer model, face-swapping roop-unleashed model, frame extraction, feature analysis, temporal modeling, Python, Transformers, PyTorch, Machine Learning, Deep Learning, sklearn, matplotlib, numpy, AI-Driven Diagnostic Assistance, Gastrointestinal Tract, stomach disease prediction, endoscopy images, CNN, ResNet-50, VGG-19, LSTM, neural network architectures.\n",
      "\n",
      "Let me know if you'd like me to help with anything else! \n",
      "\n",
      "Alternatively, I can also provide the list in a more categorized format:\n",
      "\n",
      "**Domain-specific terminology:** Deepfake detection, Machine Learning, Deep Learning, neural network architectures, temporal modeling, frame extraction, feature analysis\n",
      "**Named entities:** TimeSformer transformer model, face-swapping roop-unleashed model, Python, Transformers, PyTorch, sklearn, matplotlib, numpy, CNN, ResNet-50, VGG-19, LSTM\n",
      "**Core concepts:** AI-Driven Diagnostic Assistance, Gastrointestinal Tract, stomach disease prediction, endoscopy images \n",
      "\n",
      "Let me know if you'd like me to reformat it in any specific way!\n",
      "\n",
      "Processing Chunk 4/5\n",
      "Keywords from Chunk 4: Here are the important keywords and key phrases extracted from the text as a comma-separated list:\n",
      "\n",
      "data preprocessing, feature selection, hyperparameter tuning, regularization techniques, model optimization, Deep Learning, Machine Learning, Python, Numpy, Pandas, Neural Networks, CNN, ResNet-50, VGG-19, LSTM, sklearn, Matplotlib, Tensorflow, AI-Powered Conversational Chatbot, Large Language Models (LLMs), LLaMA-4, external API tools, OpenWeather API, Google Serper API, Langchain, Groq API, OpenWeatherMap API, Dotenv, INTERNATIONAL CONFERENCE ON INNOVATIVE RESEARCH AND DEVELOPMENT (ICIRD)\n",
      "\n",
      "Let me know if you'd like me to help with anything else! \n",
      "\n",
      "Alternatively, I can also group them by category:\n",
      "\n",
      "**Domain-specific terminology:** data preprocessing, feature selection, hyperparameter tuning, regularization techniques, model optimization, Deep Learning, Machine Learning, Neural Networks, CNN, ResNet-50, VGG-19, LSTM, Large Language Models (LLMs)\n",
      "\n",
      "**Named entities:** Python, Numpy, Pandas, sklearn, Matplotlib, Tensorflow, Langchain, Groq API, OpenWeatherMap API, Google Serper API, Dotenv, OpenWeather API, LLaMA-4\n",
      "\n",
      "**Core concepts:** AI-Powered Conversational Chatbot, INTERNATIONAL CONFERENCE ON INNOVATIVE RESEARCH AND DEVELOPMENT (ICIRD) \n",
      "\n",
      "Let me know if you'd like me to reformat it!\n",
      "\n",
      "Processing Chunk 5/5\n",
      "Keywords from Chunk 5: Here are the extracted keywords and key phrases in a comma-separated list:\n",
      "\n",
      "DIAT certified, Artificial Intelligence Professional, Programming for everyone, Getting Started with Python, Python Professional \n",
      "\n",
      "Let me reformat to remove extra space: \n",
      "DIAT certified, Artificial Intelligence Professional, Programming for everyone, Getting Started with Python, Python Professional\n",
      "\n",
      "--- All Extracted Key Phrases (Unique) ---\n",
      "AI, AI & ML, AI & ML\n",
      "\n",
      "**Projects and Courses:** Design and Development of Data Engineering Techniques for Deepfake Detection, AI-Driven Diagnostic Assistance, AI-Powered Conversational Chatbot, API, Artificial Intelligence Professional, Bachelor of Engineering, CNN, Calorie prediction model, Chemistry, Cloud Computing, Cloud Computing\n",
      "\n",
      "**Education and Institutions:** Symbiosis Institute Of Technology, Computer Networks, Computer Science, DL, Data Engineering, Data Engineering Techniques.\n",
      "\n",
      "Let me know if you'd like me to help with anything else! \n",
      "\n",
      "Alternatively, Data Science, Data Structures and Algorithms, Data Visualization, Databases, Deep Learning, Deepfake Detection, Discrete Mathematics, Dotenv, Face Swapping Tool, Fitbit activity data, Gastrointestinal Tract, Getting Started with Python, Git, GitHub, Google Serper API, Groq API, Here are the extracted keywords and key phrases in a comma-separated list:\n",
      "\n",
      "DIAT certified, Here are the extracted keywords and key phrases in a comma-separated list:\n",
      "\n",
      "Deepfake detection, Here are the important keywords and key phrases extracted from the text as a comma-separated list:\n",
      "\n",
      "C/C++, Here are the important keywords and key phrases extracted from the text as a comma-separated list:\n",
      "\n",
      "Hugging Face, Here are the important keywords and key phrases extracted from the text as a comma-separated list:\n",
      "\n",
      "data preprocessing, Hugging-face, I can also group them by category for better readability:\n",
      "\n",
      "**Tools and Technologies:** Hugging Face, I can also group them by category:\n",
      "\n",
      "**Domain-specific terminology:** data preprocessing, I can also provide the list in a more categorized format:\n",
      "\n",
      "**Domain-specific terminology:** Deepfake detection, I can also provide the list in a more categorized format:\n",
      "\n",
      "**Programming Languages:** C/C++, INTERNATIONAL CONFERENCE ON INNOVATIVE RESEARCH AND DEVELOPMENT (ICIRD)\n",
      "\n",
      "Let me know if you'd like me to help with anything else! \n",
      "\n",
      "Alternatively, INTERNATIONAL CONFERENCE ON INNOVATIVE RESEARCH AND DEVELOPMENT (ICIRD) \n",
      "\n",
      "Let me know if you'd like me to reformat it!, Java, Joblib, LLM, LLaMA-4, LLaMA-4\n",
      "\n",
      "**Core concepts:** AI-Powered Conversational Chatbot, LSTM, LSTM\n",
      "**Core concepts:** AI-Driven Diagnostic Assistance, Langchain, Large Language Models (LLMs), Large Language Models (LLMs)\n",
      "\n",
      "**Named entities:** Python, ML, ML & Data Engineering Intern.\n",
      "\n",
      "Let me know if you'd like me to help with anything else! \n",
      "\n",
      "Alternatively, ML Internship, Machine Learning, Matplotlib, NLP, NN, Neural Networks, Numpy, Object Oriented Programming, OpenWeather API, OpenWeatherMap API, Operating Systems, Pandas, Physics) \n",
      "\n",
      "Let me know if you have a preference!, Pi-labs.ai, PowerBI, Programming for everyone, PyTorch, Python, Python Professional, Python Professional \n",
      "\n",
      "Let me reformat to remove extra space: \n",
      "DIAT certified, RAG, RAG\n",
      "**Domain-specific Concepts:** deepfake detection, Random Forest, Relevant Coursework ( Databases, ResNet-50, Roop-unleashed, SQL, SQL\n",
      "**Technologies & Tools:** AI, Scikit-learn, Scikit-learn\n",
      "\n",
      "**Concepts and Techniques:** Data Engineering, Streamlit, Symbiosis Institute Of Technology, Tensorflow, TimeSformer transformer model, Timesformer Model, Transformers, VGG-19, data collection, data engineering, data processing, deepfake detection, endoscopy images, endoscopy images \n",
      "\n",
      "Let me know if you'd like me to reformat it in any specific way!, external API tools, face-swapping model, face-swapping roop-unleashed model, feature analysis, feature analysis\n",
      "**Named entities:** TimeSformer transformer model, feature selection, frame extraction, hyperparameter tuning, matplotlib, model optimization, neural network architectures, neural network architectures.\n",
      "\n",
      "Let me know if you'd like me to help with anything else! \n",
      "\n",
      "Alternatively, numpy, regularization techniques, roop-unleashed, roop-unleashed\n",
      "**Organizations:** Pi-labs.ai \n",
      "**Roles:** ML & Data Engineering Intern, sklearn, stomach disease prediction, temporal modeling, transformer-based deepfake detection model, transformer-based deepfake detection model\n",
      "**Models & Frameworks:** Timesformer Model, yt-dlp\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Flow ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the Universal Keyword Extractor!\")\n",
    "    print(\"You can provide text directly, a file path (PDF, DOCX, TXT), or a URL.\")\n",
    "    user_input = input(\"Enter your text, file path, or URL: \").strip()\n",
    "\n",
    "    # Load documents using LangChain loaders\n",
    "    loaded_docs = load_documents_from_input(user_input)\n",
    "\n",
    "    if not loaded_docs:\n",
    "        print(\"No content extracted from the provided input. Exiting.\")\n",
    "    else:\n",
    "        # Concatenate content of all loaded documents for splitting (or process them individually)\n",
    "        # For simplicity, we'll join all page_content into one string if multiple documents are returned\n",
    "        # This is common for initial processing before splitting into chunks.\n",
    "        full_article_content = \"\\n\\n\".join([doc.page_content for doc in loaded_docs])\n",
    "\n",
    "        # 4. Split the document into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        # Use the split_text method if you joined everything into one string\n",
    "        # Or, if you want to split each document individually:\n",
    "        # chunks = []\n",
    "        # for doc in loaded_docs:\n",
    "        #     chunks.extend(text_splitter.split_documents([doc]))\n",
    "        \n",
    "        # For now, let's stick to splitting the concatenated content as before\n",
    "        chunks = text_splitter.create_documents([full_article_content])\n",
    "\n",
    "        print(f\"Split article into {len(chunks)} chunks.\")\n",
    "\n",
    "        all_extracted_keywords = set()\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"\\nProcessing Chunk {i+1}/{len(chunks)}\")\n",
    "            extracted_from_chunk = keyword_extraction_chain.invoke({\"text\": chunk.page_content})\n",
    "            print(f\"Keywords from Chunk {i+1}: {extracted_from_chunk}\")\n",
    "\n",
    "            for kw in extracted_from_chunk.split(','):\n",
    "                all_extracted_keywords.add(kw.strip())\n",
    "\n",
    "        print(\"\\n--- All Extracted Key Phrases (Unique) ---\")\n",
    "        print(\", \".join(sorted(list(all_extracted_keywords))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
